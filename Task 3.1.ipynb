{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and initiate findspark\n",
    "Then import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiate SparkSession with Hive support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Check point 3\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://localhost:54310/user/hive/warehouse\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# On some clusters the following config setting may be requied\n",
    "#         .config(\"hive.metastore.uris\", \"<value>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create database capstone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|    capstone|\n",
      "|     default|\n",
      "|        nyse|\n",
      "|      office|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Hive integration\n",
    "spark.sql('show databases').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- selling_price: integer (nullable = true)\n",
      " |-- km_driven: integer (nullable = true)\n",
      " |-- StateorProvince: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- mileage: double (nullable = true)\n",
      " |-- engine: integer (nullable = true)\n",
      " |-- max_power: double (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- sold: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars = spark.read.load(\"/home/hduser/Downloads/sharedfolder/df_cars_merged.csv\", format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")\n",
    "df_cars.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Hive integration\n",
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars.write.mode(\"append\").saveAsTable(\"capstone.dfcars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+\n",
      "|_c0|year|selling_price|km_driven|     StateorProvince|      City|  fuel|seller_type|transmission|       owner|mileage|engine|max_power|seats|sold|Region|     brand|\n",
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+\n",
      "|  0|2014|       450000|   145500|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   23.4|  1248|     74.0|    5|   Y|  East|    MARUTI|\n",
      "|  1|2019|      1149000|     5000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|   17.0|  1591|    121.3|    5|   Y|  East|   HYUNDAI|\n",
      "|  2|2017|       600000|    25000|District of Columbia|Washington|Petrol| Individual|      Manual| Third Owner|  18.16|  1196|     86.8|    5|   Y|  East|      FORD|\n",
      "|  3|2016|       540000|    40000|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   22.0|  1498|  108.495|    5|   Y|  East|VOLKSWAGEN|\n",
      "|  4|2015|       630000|   135000|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   25.1|  1498|     98.6|    5|   Y|  East|     HONDA|\n",
      "|  5|2018|       448000|    30000|District of Columbia|Washington|Petrol| Individual|   Automatic| First Owner|  20.51|   998|     67.0|    5|   Y|  East|    MARUTI|\n",
      "|  6|2009|       185000|    77000|District of Columbia|Washington|Petrol|     Dealer|      Manual|Second Owner|  21.79|   998|    67.05|    5|   Y|  East|    MARUTI|\n",
      "|  7|2010|       200000|   100000|District of Columbia|Washington|Diesel| Individual|      Manual|Second Owner|   18.8|  1248|     90.0|    5|   Y|  East|      TATA|\n",
      "|  8|2019|       615000|    10000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|  21.21|  1197|     81.8|    5|   Y|  East|    MARUTI|\n",
      "|  9|2014|       400000|    70000|District of Columbia|Washington|Diesel| Individual|      Manual| Third Owner|  20.45|  1461|     83.8|    5|   Y|  East|    NISSAN|\n",
      "| 10|2015|       700000|   110000|District of Columbia|Washington|Diesel| Individual|      Manual|Second Owner|  20.77|  1248|    88.76|    7|   Y|  East|    MARUTI|\n",
      "| 11|2019|       690000|     1303|District of Columbia|Washington|Petrol|     Dealer|      Manual| First Owner|   17.8|  1198|     86.7|    5|   Y|  East|     HONDA|\n",
      "| 12|2013|       625000|    39000|District of Columbia|Washington|Petrol|     Dealer|   Automatic| First Owner|  14.28|  1798|   138.03|    5|   Y|  East|    TOYOTA|\n",
      "| 13|2018|      3400000|    22000|District of Columbia|Washington|Diesel|     Dealer|   Automatic| First Owner|   18.0|  1969|    190.0|    5|   Y|  East|     VOLVO|\n",
      "| 14|2010|       975000|    72200|District of Columbia|Washington|Petrol|     Dealer|   Automatic|Second Owner|   10.8|  2497|    150.0|    5|   Y|  East|       BMW|\n",
      "| 15|2016|       805000|    49900|District of Columbia|Washington|Petrol|     Dealer|      Manual| First Owner|   16.3|  1797|    147.5|    5|   Y|  East|   HYUNDAI|\n",
      "| 16|2011|       375000|    49000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|  15.04|  1598|    103.6|    5|   Y|  East|VOLKSWAGEN|\n",
      "| 17|2016|      2900000|    12000|District of Columbia|Washington|Diesel|     Dealer|   Automatic| First Owner|  18.12|  1995|    190.0|    5|   Y|  East|       BMW|\n",
      "| 18|2015|       390000|    50000|District of Columbia|Washington|Petrol| Individual|   Automatic|Second Owner|  20.51|   998|     67.0|    5|   Y|  East|    MARUTI|\n",
      "| 19|1999|        40000|    40000|District of Columbia|Washington|Petrol| Individual|      Manual|Second Owner|   16.1|   796|     37.0|    4|   Y|  East|    MARUTI|\n",
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from capstone.dfcars').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "|capstone|   dfcars|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables from capstone').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('use capstone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('year', 'int'),\n",
       " ('selling_price', 'int'),\n",
       " ('km_driven', 'int'),\n",
       " ('StateorProvince', 'string'),\n",
       " ('City', 'string'),\n",
       " ('fuel', 'string'),\n",
       " ('seller_type', 'string'),\n",
       " ('transmission', 'string'),\n",
       " ('owner', 'string'),\n",
       " ('mileage', 'double'),\n",
       " ('engine', 'int'),\n",
       " ('max_power', 'double'),\n",
       " ('seats', 'int'),\n",
       " ('sold', 'string'),\n",
       " ('Region', 'string'),\n",
       " ('brand', 'string')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features=['year','selling_price','km_driven','mileage','engine','max_power','seats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|              year|    selling_price|        km_driven|           mileage|            engine|        max_power|             seats|\n",
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|              7906|             7906|             7906|              7906|              7906|             7906|              7906|\n",
      "|   mean|2013.9839362509485| 649813.720844928|69188.65975208703|19.419860865165695|1458.7088287376675|91.58737351378637|5.4163926132051605|\n",
      "| stddev|3.8636953387034967|813582.7483541325|56792.29634331763| 4.036263200758886|  503.893056850139|35.74721608448376|0.9592082121984603|\n",
      "|    min|              1994|            29999|                1|               0.0|               624|             32.8|                 2|\n",
      "|    max|              2020|         10000000|          2360457|              42.0|              3604|            400.0|                14|\n",
      "+-------+------------------+-----------------+-----------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cars.describe(numerical_features).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+---------+---------------+----+----+-----------+------------+-----+-------+------+---------+-----+----+------+-----+\n",
      "|_c0|year|selling_price|km_driven|StateorProvince|City|fuel|seller_type|transmission|owner|mileage|engine|max_power|seats|sold|Region|brand|\n",
      "+---+----+-------------+---------+---------------+----+----+-----------+------------+-----+-------+------+---------+-----+----+------+-----+\n",
      "|  0|   0|            0|        0|              0|   0|   0|          0|           0|    0|      0|     0|        0|    0|   0|     0|    0|\n",
      "+---+----+-------------+---------+---------------+----+----+-----------+------------+-----+-------+------+---------+-----+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_cars.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_cars.columns]).show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are no missing values found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds(df):\n",
    "    bounds = {\n",
    "        c: dict(\n",
    "            zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "        )\n",
    "        for c,d in zip(df.columns, df.dtypes) if d[1] == \"int\" or d[1]==\"double\"\n",
    "    }\n",
    "\n",
    "    for c in bounds:\n",
    "        iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "        bounds[c]['min'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "        bounds[c]['max'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "\n",
    "    return bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_c0': {'max': 11859.5, 'min': -3952.5, 'q1': 1977.0, 'q3': 5930.0},\n",
       " 'engine': {'max': 2159.5, 'min': 619.5, 'q1': 1197.0, 'q3': 1582.0},\n",
       " 'km_driven': {'max': 187500.0, 'min': -56500.0, 'q1': 35000.0, 'q3': 96000.0},\n",
       " 'max_power': {'max': 152.925,\n",
       "  'min': 17.124999999999993,\n",
       "  'q1': 68.05,\n",
       "  'q3': 102.0},\n",
       " 'mileage': {'max': 30.63, 'min': 8.470000000000002, 'q1': 16.78, 'q3': 22.32},\n",
       " 'seats': {'max': 5.0, 'min': 5.0, 'q1': 5.0, 'q3': 5.0},\n",
       " 'selling_price': {'max': 1320000.0,\n",
       "  'min': -360000.0,\n",
       "  'q1': 270000.0,\n",
       "  'q3': 690000.0},\n",
       " 'year': {'max': 2024.5, 'min': 2004.5, 'q1': 2012.0, 'q3': 2017.0}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_bounds(df_cars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "def flag_outliers(df, id_col):\n",
    "    bounds = calculate_bounds(df)\n",
    "    outliers = {}\n",
    "\n",
    "    return df.select(c, id_col,\n",
    "        *[\n",
    "            f.when(\n",
    "                ~f.col(c).between(bounds[c]['min'], bounds[c]['max']),\"yes\"\n",
    "            ).otherwise(\"no\").alias(c+'_outlier')\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+\n",
      "|year|year|year_outlier|\n",
      "+----+----+------------+\n",
      "|2014|2014|          no|\n",
      "|2019|2019|          no|\n",
      "|2017|2017|          no|\n",
      "|2016|2016|          no|\n",
      "|2015|2015|          no|\n",
      "|2018|2018|          no|\n",
      "|2009|2009|          no|\n",
      "|2010|2010|          no|\n",
      "|2019|2019|          no|\n",
      "|2014|2014|          no|\n",
      "|2015|2015|          no|\n",
      "|2019|2019|          no|\n",
      "|2013|2013|          no|\n",
      "|2018|2018|          no|\n",
      "|2010|2010|          no|\n",
      "|2016|2016|          no|\n",
      "|2011|2011|          no|\n",
      "|2016|2016|          no|\n",
      "|2015|2015|          no|\n",
      "|1999|1999|         yes|\n",
      "+----+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+-------------+---------------------+\n",
      "|selling_price|selling_price|selling_price_outlier|\n",
      "+-------------+-------------+---------------------+\n",
      "|       450000|       450000|                   no|\n",
      "|      1149000|      1149000|                   no|\n",
      "|       600000|       600000|                   no|\n",
      "|       540000|       540000|                   no|\n",
      "|       630000|       630000|                   no|\n",
      "|       448000|       448000|                   no|\n",
      "|       185000|       185000|                   no|\n",
      "|       200000|       200000|                   no|\n",
      "|       615000|       615000|                   no|\n",
      "|       400000|       400000|                   no|\n",
      "|       700000|       700000|                   no|\n",
      "|       690000|       690000|                   no|\n",
      "|       625000|       625000|                   no|\n",
      "|      3400000|      3400000|                  yes|\n",
      "|       975000|       975000|                   no|\n",
      "|       805000|       805000|                   no|\n",
      "|       375000|       375000|                   no|\n",
      "|      2900000|      2900000|                  yes|\n",
      "|       390000|       390000|                   no|\n",
      "|        40000|        40000|                   no|\n",
      "+-------------+-------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+-----------------+\n",
      "|km_driven|km_driven|km_driven_outlier|\n",
      "+---------+---------+-----------------+\n",
      "|   145500|   145500|               no|\n",
      "|     5000|     5000|               no|\n",
      "|    25000|    25000|               no|\n",
      "|    40000|    40000|               no|\n",
      "|   135000|   135000|               no|\n",
      "|    30000|    30000|               no|\n",
      "|    77000|    77000|               no|\n",
      "|   100000|   100000|               no|\n",
      "|    10000|    10000|               no|\n",
      "|    70000|    70000|               no|\n",
      "|   110000|   110000|               no|\n",
      "|     1303|     1303|               no|\n",
      "|    39000|    39000|               no|\n",
      "|    22000|    22000|               no|\n",
      "|    72200|    72200|               no|\n",
      "|    49900|    49900|               no|\n",
      "|    49000|    49000|               no|\n",
      "|    12000|    12000|               no|\n",
      "|    50000|    50000|               no|\n",
      "|    40000|    40000|               no|\n",
      "+---------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-------+---------------+\n",
      "|mileage|mileage|mileage_outlier|\n",
      "+-------+-------+---------------+\n",
      "|   23.4|   23.4|             no|\n",
      "|   17.0|   17.0|             no|\n",
      "|  18.16|  18.16|             no|\n",
      "|   22.0|   22.0|             no|\n",
      "|   25.1|   25.1|             no|\n",
      "|  20.51|  20.51|             no|\n",
      "|  21.79|  21.79|             no|\n",
      "|   18.8|   18.8|             no|\n",
      "|  21.21|  21.21|             no|\n",
      "|  20.45|  20.45|             no|\n",
      "|  20.77|  20.77|             no|\n",
      "|   17.8|   17.8|             no|\n",
      "|  14.28|  14.28|             no|\n",
      "|   18.0|   18.0|             no|\n",
      "|   10.8|   10.8|             no|\n",
      "|   16.3|   16.3|             no|\n",
      "|  15.04|  15.04|             no|\n",
      "|  18.12|  18.12|             no|\n",
      "|  20.51|  20.51|             no|\n",
      "|   16.1|   16.1|             no|\n",
      "+-------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+------+--------------+\n",
      "|engine|engine|engine_outlier|\n",
      "+------+------+--------------+\n",
      "|  1248|  1248|            no|\n",
      "|  1591|  1591|            no|\n",
      "|  1196|  1196|            no|\n",
      "|  1498|  1498|            no|\n",
      "|  1498|  1498|            no|\n",
      "|   998|   998|            no|\n",
      "|   998|   998|            no|\n",
      "|  1248|  1248|            no|\n",
      "|  1197|  1197|            no|\n",
      "|  1461|  1461|            no|\n",
      "|  1248|  1248|            no|\n",
      "|  1198|  1198|            no|\n",
      "|  1798|  1798|            no|\n",
      "|  1969|  1969|            no|\n",
      "|  2497|  2497|           yes|\n",
      "|  1797|  1797|            no|\n",
      "|  1598|  1598|            no|\n",
      "|  1995|  1995|            no|\n",
      "|   998|   998|            no|\n",
      "|   796|   796|            no|\n",
      "+------+------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+---------+-----------------+\n",
      "|max_power|max_power|max_power_outlier|\n",
      "+---------+---------+-----------------+\n",
      "|     74.0|     74.0|               no|\n",
      "|    121.3|    121.3|               no|\n",
      "|     86.8|     86.8|               no|\n",
      "|  108.495|  108.495|               no|\n",
      "|     98.6|     98.6|               no|\n",
      "|     67.0|     67.0|               no|\n",
      "|    67.05|    67.05|               no|\n",
      "|     90.0|     90.0|               no|\n",
      "|     81.8|     81.8|               no|\n",
      "|     83.8|     83.8|               no|\n",
      "|    88.76|    88.76|               no|\n",
      "|     86.7|     86.7|               no|\n",
      "|   138.03|   138.03|               no|\n",
      "|    190.0|    190.0|              yes|\n",
      "|    150.0|    150.0|               no|\n",
      "|    147.5|    147.5|               no|\n",
      "|    103.6|    103.6|               no|\n",
      "|    190.0|    190.0|              yes|\n",
      "|     67.0|     67.0|               no|\n",
      "|     37.0|     37.0|               no|\n",
      "+---------+---------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----+-----+-------------+\n",
      "|seats|seats|seats_outlier|\n",
      "+-----+-----+-------------+\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    7|    7|          yes|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    5|    5|           no|\n",
      "|    4|    4|          yes|\n",
      "+-----+-----+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in numerical_features:\n",
    "    flag_outliers(df_cars,c).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We observe outliers in the dataset and therefore they need to be handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+------------+\n",
      "|_c0|year|selling_price|km_driven|     StateorProvince|      City|  fuel|seller_type|transmission|       owner|mileage|engine|max_power|seats|sold|Region|     brand|fuel_numeric|\n",
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+------------+\n",
      "|  0|2014|       450000|   145500|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   23.4|  1248|     74.0|    5|   Y|  East|    MARUTI|         0.0|\n",
      "|  1|2019|      1149000|     5000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|   17.0|  1591|    121.3|    5|   Y|  East|   HYUNDAI|         1.0|\n",
      "|  2|2017|       600000|    25000|District of Columbia|Washington|Petrol| Individual|      Manual| Third Owner|  18.16|  1196|     86.8|    5|   Y|  East|      FORD|         1.0|\n",
      "|  3|2016|       540000|    40000|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   22.0|  1498|  108.495|    5|   Y|  East|VOLKSWAGEN|         0.0|\n",
      "|  4|2015|       630000|   135000|District of Columbia|Washington|Diesel| Individual|      Manual| First Owner|   25.1|  1498|     98.6|    5|   Y|  East|     HONDA|         0.0|\n",
      "|  5|2018|       448000|    30000|District of Columbia|Washington|Petrol| Individual|   Automatic| First Owner|  20.51|   998|     67.0|    5|   Y|  East|    MARUTI|         1.0|\n",
      "|  6|2009|       185000|    77000|District of Columbia|Washington|Petrol|     Dealer|      Manual|Second Owner|  21.79|   998|    67.05|    5|   Y|  East|    MARUTI|         1.0|\n",
      "|  7|2010|       200000|   100000|District of Columbia|Washington|Diesel| Individual|      Manual|Second Owner|   18.8|  1248|     90.0|    5|   Y|  East|      TATA|         0.0|\n",
      "|  8|2019|       615000|    10000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|  21.21|  1197|     81.8|    5|   Y|  East|    MARUTI|         1.0|\n",
      "|  9|2014|       400000|    70000|District of Columbia|Washington|Diesel| Individual|      Manual| Third Owner|  20.45|  1461|     83.8|    5|   Y|  East|    NISSAN|         0.0|\n",
      "| 10|2015|       700000|   110000|District of Columbia|Washington|Diesel| Individual|      Manual|Second Owner|  20.77|  1248|    88.76|    7|   Y|  East|    MARUTI|         0.0|\n",
      "| 11|2019|       690000|     1303|District of Columbia|Washington|Petrol|     Dealer|      Manual| First Owner|   17.8|  1198|     86.7|    5|   Y|  East|     HONDA|         1.0|\n",
      "| 12|2013|       625000|    39000|District of Columbia|Washington|Petrol|     Dealer|   Automatic| First Owner|  14.28|  1798|   138.03|    5|   Y|  East|    TOYOTA|         1.0|\n",
      "| 13|2018|      3400000|    22000|District of Columbia|Washington|Diesel|     Dealer|   Automatic| First Owner|   18.0|  1969|    190.0|    5|   Y|  East|     VOLVO|         0.0|\n",
      "| 14|2010|       975000|    72200|District of Columbia|Washington|Petrol|     Dealer|   Automatic|Second Owner|   10.8|  2497|    150.0|    5|   Y|  East|       BMW|         1.0|\n",
      "| 15|2016|       805000|    49900|District of Columbia|Washington|Petrol|     Dealer|      Manual| First Owner|   16.3|  1797|    147.5|    5|   Y|  East|   HYUNDAI|         1.0|\n",
      "| 16|2011|       375000|    49000|District of Columbia|Washington|Petrol| Individual|      Manual| First Owner|  15.04|  1598|    103.6|    5|   Y|  East|VOLKSWAGEN|         1.0|\n",
      "| 17|2016|      2900000|    12000|District of Columbia|Washington|Diesel|     Dealer|   Automatic| First Owner|  18.12|  1995|    190.0|    5|   Y|  East|       BMW|         0.0|\n",
      "| 18|2015|       390000|    50000|District of Columbia|Washington|Petrol| Individual|   Automatic|Second Owner|  20.51|   998|     67.0|    5|   Y|  East|    MARUTI|         1.0|\n",
      "| 19|1999|        40000|    40000|District of Columbia|Washington|Petrol| Individual|      Manual|Second Owner|   16.1|   796|     37.0|    4|   Y|  East|    MARUTI|         1.0|\n",
      "+---+----+-------------+---------+--------------------+----------+------+-----------+------------+------------+-------+------+---------+-----+----+------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"fuel\", outputCol=\"fuel_numeric\")\n",
    "indexed = indexer.fit(df_cars).transform(df_cars)\n",
    "indexed.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid param value given for param \"outputCol\". Could not convert <class 'list'> to string type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36mtoString\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not convert %s to string type\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert <class 'list'> to string type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-807c4478ab3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder = OneHotEncoder(inputCol=[\"transmission\"],\n\u001b[0;32m----> 2\u001b[0;31m                        outputCol=[\"transmission_vect\"])\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropLast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mkeyword_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method %s forces keyword arguments.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/feature.py\u001b[0m in \u001b[0;36msetParams\u001b[0;34m(self, dropLast, inputCol, outputCol)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \"\"\"\n\u001b[1;32m   1541\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1.4.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/param/__init__.py\u001b[0m in \u001b[0;36m_set\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeConverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid param value given for param \"%s\". %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid param value given for param \"outputCol\". Could not convert <class 'list'> to string type"
     ]
    }
   ],
   "source": [
    "encoder = OneHotEncoder(inputCol=[\"transmission\"],\n",
    "                       outputCol=[\"transmission_vect\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=['year','selling_price','km_driven','StateorProvince','City','fuel','seller_type','transmission', 'owner','mileage','engine','max_power','seats','sold','Region','brand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- selling_price: integer (nullable = true)\n",
      " |-- km_driven: integer (nullable = true)\n",
      " |-- StateorProvince: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- seller_type: string (nullable = true)\n",
      " |-- transmission: string (nullable = true)\n",
      " |-- owner: string (nullable = true)\n",
      " |-- mileage: double (nullable = true)\n",
      " |-- engine: integer (nullable = true)\n",
      " |-- max_power: double (nullable = true)\n",
      " |-- seats: integer (nullable = true)\n",
      " |-- sold: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_data = df_cars.select(col(\"selling_price\").alias(\"label\"), *features)\n",
    "lr_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Data type StringType is not supported.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o300.transform.\n: java.lang.IllegalArgumentException: Data type StringType is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:121)\n\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$transformSchema$1.apply(VectorAssembler.scala:117)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:117)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:54)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-52cee85cf7a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mva_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorAssembler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Data type StringType is not supported.'"
     ]
    }
   ],
   "source": [
    "va_data = vectorAssembler.transform(lr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
